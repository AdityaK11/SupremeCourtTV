# -*- coding: utf-8 -*-
"""Copy of ASR_with_SpeakerDiarization.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12z7Q3JX8rm7vTtaqMaCHaAWa41wT_WYr

# Automatic Speech Recognition with Speaker Diarization
"""

# ## Install dependencies
# !pip install wget
# !apt-get install sox libsndfile1 ffmpeg
# !pip install text-unidecode

# # ## Install NeMo
# BRANCH = 'r1.23.0'
# !python -m pip install git+https://github.com/NVIDIA/NeMo.git@$r1.23.0#egg=nemo_toolkit[asr]

# ## Install TorchAudio
# !pip install torchaudio -f https://download.pytorch.org/whl/torch_stable.html
# conda install -c conda-forge cudatoolkit=11.2


import nemo.collections.asr as nemo_asr
import numpy as np
from IPython.display import Audio, display
import librosa
import os
import wget
import matplotlib.pyplot as plt

import nemo
import glob

import pprint
pp = pprint.PrettyPrinter(indent=4)

case_name = 'case3'

ROOT = os.getcwd()
data_dir = f'/home/akathpalia/data/SupremeCourtTv/{case_name}'
os.makedirs(data_dir, exist_ok=True)

AUDIO_FILENAME = os.path.join(data_dir,f'{case_name}.wav')

audio_file_list = glob.glob(f"{data_dir}/*.wav")
print("Input audio file list: \n", audio_file_list)

signal, sample_rate = librosa.load(AUDIO_FILENAME, sr=None)

from omegaconf import OmegaConf
import shutil
DOMAIN_TYPE = "meeting" 
CONFIG_FILE_NAME = f"diar_infer_{DOMAIN_TYPE}.yaml"

CONFIG_URL = f"https://raw.githubusercontent.com/NVIDIA/NeMo/main/examples/speaker_tasks/diarization/conf/inference/{CONFIG_FILE_NAME}"

if not os.path.exists(os.path.join(data_dir,CONFIG_FILE_NAME)):
    CONFIG = wget.download(CONFIG_URL, data_dir)
else:
    CONFIG = os.path.join(data_dir,CONFIG_FILE_NAME)

cfg = OmegaConf.load(CONFIG)
print(OmegaConf.to_yaml(cfg))

"""Speaker Diarization scripts commonly expects following arguments:
1. manifest_filepath : Path to manifest file containing json lines of format: `{"audio_filepath": "/path/to/audio_file", "offset": 0, "duration": null, "label": "infer", "text": "-", "num_speakers": null, "rttm_filepath": "/path/to/rttm/file", "uem_filepath"="/path/to/uem/filepath"}`
2. out_dir : directory where outputs and intermediate files are stored.
3. oracle_vad: If this is true then we extract speech activity labels from rttm files, if False then either
4. vad.model_path or external_manifestpath containing speech activity labels has to be passed.

Mandatory fields are `audio_filepath`, `offset`, `duration`, `label` and `text`. For the rest if you would like to evaluate with a known number of speakers pass the value else `null`. If you would like to score the system with known rttms then that should be passed as well, else `null`. uem file is used to score only part of your audio for evaluation purposes, hence pass if you would like to evaluate on it else `null`.


**Note:** we expect audio and corresponding RTTM to have **same base name** and the name should be **unique**.

For example: if audio file name is **test_an4**.wav, if provided we expect corresponding rttm file name to be **test_an4**.rttm (note the matching **test_an4** base name)

Lets create a manifest file with the an4 audio and rttm available. If you have more than one file you may also use the script `NeMo/scripts/speaker_tasks/pathfiles_to_diarize_manifest.py` to generate a manifest file from a list of audio files. In addition, you can optionally include rttm files to evaluate the diarization results.
"""

# Create a manifest file for input with below format.
# {"audio_filepath": "/path/to/audio_file", "offset": 0, "duration": null, "label": "infer", "text": "-",
# "num_speakers": null, "rttm_filepath": "/path/to/rttm/file", "uem_filepath"="/path/to/uem/filepath"}
import json
meta = {
    'audio_filepath': AUDIO_FILENAME,
    'offset': 0,
    'duration':None,
    'label': 'infer',
    'text': '-',
    'num_speakers': None,
    'rttm_filepath': None,
    'uem_filepath' : None
}
with open(os.path.join(data_dir,'input_manifest.json'),'w') as fp:
    json.dump(meta,fp)
    fp.write('\n')

cfg.diarizer.manifest_filepath = os.path.join(data_dir,'input_manifest.json')
# !cat {cfg.diarizer.manifest_filepath}

"""Let's set the parameters required for diarization. In this tutorial, we obtain voice activity labels from ASR, which is set through parameter `cfg.diarizer.asr.parameters.asr_based_vad`."""

pretrained_speaker_model='titanet_large'
cfg.diarizer.manifest_filepath = cfg.diarizer.manifest_filepath
cfg.diarizer.out_dir = data_dir #Directory to store intermediate files and prediction outputs
cfg.diarizer.speaker_embeddings.model_path = pretrained_speaker_model
cfg.diarizer.clustering.parameters.oracle_num_speakers=False

# Using Neural VAD and Conformer ASR
cfg.diarizer.vad.model_path = 'vad_multilingual_marblenet'
cfg.diarizer.asr.model_path = 'stt_en_conformer_ctc_large'
cfg.diarizer.oracle_vad = False # ----> Not using oracle VAD
cfg.diarizer.asr.parameters.asr_based_vad = False

"""### Run ASR and get word timestamps
Before we run speaker diarization, we should run ASR and get the ASR output to generate decoded words and timestamps for those words. Let's import `ASRDecoderTimeStamps` class and create `asr_decoder_ts` instance that returns an ASR model. Using this ASR model, the following two variables are obtained from `asr_decoder_ts.run_ASR()` function.

- word_hyp Dict[str, List[str]]: contains the sequence of words.
- word_ts_hyp Dict[str, List[int]]: contains frame level index of the start and the end of each word.
"""

from nemo.collections.asr.parts.utils.decoder_timestamps_utils import ASRDecoderTimeStamps
asr_decoder_ts = ASRDecoderTimeStamps(cfg.diarizer)
asr_model = asr_decoder_ts.set_asr_model()
word_hyp, word_ts_hyp = asr_decoder_ts.run_ASR(asr_model)

print("Decoded word output dictionary: \n", word_hyp[f'{case_name}'])
print("Word-level timestamps dictionary: \n", word_ts_hyp[f'{case_name}'])

"""Let's create an instance `asr_diar_offline` from OfflineDiarWithASR class, which matches diarization results with ASR outputs. We pass ``cfg.diarizer`` to setup the parameters for both ASR and diarization. We also set `word_ts_anchor_offset` variable that determines the anchor position of each word. Here, we use the default value from `asr_decoder_ts` instance."""

from nemo.collections.asr.parts.utils.diarization_utils import OfflineDiarWithASR
asr_diar_offline = OfflineDiarWithASR(cfg.diarizer)
asr_diar_offline.word_ts_anchor_offset = asr_decoder_ts.word_ts_anchor_offset

"""`asr_diar_offline` instance is now ready. As a next step, we run diarization.

### Run diarization with the extracted word timestamps

Now that all the components for diarization is ready, let's run diarization by calling `run_diarization()` function. `run_diarization()` will return two different variables : `diar_hyp` and `diar_score`. `diar_hyp` is diarization inference result which is written in `[start time] [end time] [speaker]` format. `diar_score` contains `None` since we did not provide `rttm_filepath` in the input manifest file.
"""

diar_hyp, diar_score = asr_diar_offline.run_diarization(cfg, word_ts_hyp)
print(diar_hyp)
print("Diarization hypothesis output: \n", diar_hyp[f'{case_name}'])

"""`run_diarization()` function also creates `an4_diarize_test.rttm` file. Let's check what is written in this `rttm` file."""

def read_file(path_to_file):
    with open(path_to_file) as f:
        contents = f.read().splitlines()
    return contents

predicted_speaker_label_rttm_path = f"{data_dir}/pred_rttms/{case_name}.rttm"
pred_rttm = read_file(predicted_speaker_label_rttm_path)

pp.pprint(pred_rttm)

from nemo.collections.asr.parts.utils.speaker_utils import rttm_to_labels
pred_labels = rttm_to_labels(predicted_speaker_label_rttm_path)

"""### Check the speaker-labeled ASR transcription output

Now we've done all the processes for running ASR and diarization, let's match the diarization result with the ASR result and get the final output. `get_transcript_with_speaker_labels()` function in `asr_diar_offline` matches diarization output `diar_hyp` with `word_hyp` using the timestamp information from `word_ts_hyp`.
"""

trans_info_dict = asr_diar_offline.get_transcript_with_speaker_labels(diar_hyp, word_hyp, word_ts_hyp)

"""After running `get_transcript_with_speaker_labels()` function, the transcription output will be located in `./pred_rttms` folder, which shows **start time to end time of the utterance, speaker ID, and words spoken** during the notified time."""

transcription_path_to_file = f"{data_dir}/pred_rttms/{case_name}.txt"
transcript = read_file(transcription_path_to_file)
pp.pprint(transcript)

"""Another output is transcription output in JSON format, which is saved in `./pred_rttms/an4_diarize_test.json`.

In the JSON format output, we include information such as **transcription, estimated number of speakers (variable named `speaker_count`), start and end time of each word and most importantly, speaker label for each word.**
"""

transcription_path_to_file = f"{data_dir}/pred_rttms/{case_name}.json"
json_contents = read_file(transcription_path_to_file)
pp.pprint(json_contents)

""" ##################  Speaker matching  ###############"""
pdf_path = f'{data_dir}/{case_name}.pdf'

import fitz  # PyMuPDF
import re

def extract_speakers_and_sentences(pdf_path):
    # Open the PDF
    doc = fitz.open(pdf_path)

    # Pattern to identify speakers (e.g., "Justice Thomas:")
    speaker_pattern = re.compile(r'([A-Z][A-Z\s.]+):')

    # Pattern to remove numbering (e.g., "1 2 3 ...")
    numbering_pattern = re.compile(r'(\d+\s*)+')

    speakers_and_sentences = {}
    current_speaker = None
    current_paragraph = []

    # Iterate through each page and extract text
    for page in doc:
        text = page.get_text("text")
        lines = text.split('\n')

        for line in lines:
            # Check if the line contains a speaker
            match = speaker_pattern.match(line)
            if match:
                print(f"Matched Speaker: {match.group()}")  # Debugging
                if current_speaker:
                    # Save the current paragraph for the current speaker
                    if current_speaker not in speakers_and_sentences:
                        speakers_and_sentences[current_speaker] = []
                    paragraph = ' '.join(current_paragraph)
                    # Split the paragraph into sentences and remove numbering
                    sentences = re.split(r'(?<=[.!?])\s+', paragraph)
                    sentences = [numbering_pattern.sub('', sentence).strip() for sentence in sentences if sentence.strip()]
                    # Remove unwanted spaces
                    sentences = [' '.join(sentence.split()) for sentence in sentences]
                    speakers_and_sentences[current_speaker].extend(sentences)
                # Start a new paragraph for the new speaker
                current_speaker = match.group().strip(':')
                current_paragraph = [line[len(current_speaker) + 2:].strip()]
            elif current_speaker:
                # Continue accumulating lines for the current speaker
                current_paragraph.append(line.strip())

    # Save the last paragraph for the last speaker
    if current_speaker:
        if current_speaker not in speakers_and_sentences:
            speakers_and_sentences[current_speaker] = []
        paragraph = ' '.join(current_paragraph)
        # Split the paragraph into sentences and remove numbering
        sentences = re.split(r'(?<=[.!?])\s+', paragraph)
        sentences = [numbering_pattern.sub('', sentence).strip() for sentence in sentences if sentence.strip()]
        # Remove unwanted spaces
        sentences = [' '.join(sentence.split()) for sentence in sentences]
        speakers_and_sentences[current_speaker].extend(sentences)

    # Remove unwanted entries like "Pages", "Place", and "Date"
    for unwanted_key in ["Pages", "Place", "Date", "APPEARANCES", "PAGE"]:
        speakers_and_sentences.pop(unwanted_key, None)

    return speakers_and_sentences

original_speakers = extract_speakers_and_sentences(pdf_path)
print(original_speakers)

# Create a new dictionary with cleaned keys
cleaned_speakers = {}

for key, value in original_speakers.items():
    cleaned_key = key.strip()  # Remove leading and trailing spaces
    if cleaned_key in cleaned_speakers:
        cleaned_speakers[cleaned_key].extend(value)
    else:
        cleaned_speakers[cleaned_key] = value

# Print the cleaned dictionary
for speaker, content in cleaned_speakers.items():
    print(f"{speaker}: {content}")

import json
from collections import defaultdict
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# Load NeMo transcript
with open(f'{data_dir}/pred_rttms/{case_name}.json', 'r') as file:
    nemo_transcript = json.load(file)

# Cluster lines spoken by each NeMo speaker
nemo_clusters = defaultdict(list)
for entry in nemo_transcript['sentences']:
    nemo_clusters[entry['speaker']].append(entry['text'])

# Keep original speakers' sentences as lists (no need to join them into a big string)
for key, value in cleaned_speakers.items():
    print(key + ':', value)

print(cleaned_speakers)

# Flatten the list of sentences for each NeMo speaker for vectorization
nemo_sentences = [sentence for sentences in nemo_clusters.values() for sentence in sentences]
vectorizer = TfidfVectorizer()
nemo_sentence_vecs = vectorizer.fit_transform(nemo_sentences)

# Flatten the list of sentences for each original speaker for vectorization
original_sentences = [sentence for sentences in cleaned_speakers.values() for sentence in sentences]
original_sentence_vecs = vectorizer.transform(original_sentences)


# Convert clusters to strings
for speaker in nemo_clusters:
    nemo_clusters[speaker] = ' '.join(nemo_clusters[speaker])

# Convert original speakers to strings
for speaker in cleaned_speakers:
    cleaned_speakers[speaker] = ' '.join(cleaned_speakers[speaker])

# Now you can proceed with the TF-IDF vectorization and similarity computation
vectorizer = TfidfVectorizer(min_df=1)
nemo_speaker_vecs = vectorizer.fit_transform(nemo_clusters.values())
original_speaker_vecs = vectorizer.transform(cleaned_speakers.values())


similarity_scores = cosine_similarity(nemo_speaker_vecs, original_speaker_vecs)

# Match NeMo speakers to original speakers
matches = {}
for nemo_idx, scores in enumerate(similarity_scores):
    best_match_idx = scores.argmax()
    nemo_speaker = list(nemo_clusters.keys())[nemo_idx]
    matched_original_speaker = list(cleaned_speakers.keys())[best_match_idx]
    matches[nemo_speaker] = matched_original_speaker

print(matches)
# Replace NeMo speaker IDs with actual speaker names in the transcript
for entry in nemo_transcript['sentences']:
    entry['speaker'] = matches.get(entry['speaker'], 'Unmatched Speaker')

# Print updated transcript
print(nemo_transcript['sentences'])

with open(f'{data_dir}/modified_{case_name}.json', 'w') as file:
    json.dump(nemo_transcript, file, indent=4)
